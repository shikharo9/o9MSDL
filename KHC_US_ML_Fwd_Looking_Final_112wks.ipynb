{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c4ff68-69a0-4343-acf7-af197c7baf0b",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This cell is NOT editable. Overwrite variables on your own discretion.\n",
    "# Any changes other than the script code will NOT BE SAVED!\n",
    "# All cells are assumed to be script code cells, unless explictly tagged as 'o9_ignore'\n",
    "\n",
    "connect_str = \"DefaultEndpointsProtocol=https;AccountName=khazeus2o9devdsmlstg001;AccountKey=+4OpyWQ4ka65EPThoCs4uIvsqkufeBrHiSKQ7vXXQHCh+L6dc0gr0SWnaTi8H/PvRqdLyl1KFVWoy2OixusLwA==;EndpointSuffix=core.windows.net\"\n",
    "destination_name = \"prophesy-preprod-ads\"\n",
    "ADS = \"Yes\"\n",
    "Pickle = \"Yes\"\n",
    "Folder_name = \"BatchRun/\"\n",
    "Number_Of_Weeks = \"116\"\n",
    "RunType = \"Forward\"\n",
    "Parquet_subfolder_prefix = \"GenerationWeek_\"\n",
    "Pickle_subfolder_prefix = \"ML_Training_models_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a9f647-f265-4486-9dff-aa5e49a62aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice Dimension Attributes defined in the plugin. Please check all queries and replace <KEY HERE> with a valid name.\n",
    "# For example: If slice is defined by Version.[Version Name] and Time.[Month]\n",
    "# input_df = ibpl Select ([Version].[Version Name].[<KEY HERE>] * [Time].[Month].[<KEY HERE>] * [Item].[Item Number]) on row, ({Measure.[M1], Measure.[M2]}) on column limit 5000;\n",
    "#                             update <KEY HERE> to valid names\n",
    "# input_df = ibpl Select ([Version].[Version Name].[CurrentWorkingView] * [Time].[Month].[January] * [Item].[Item Number]) on row, ({Measure.[M1], Measure.[M2]}) on column limit 5000;\n",
    "\n",
    "_shipment_df = \"Select ([Version].[Version Name].[CurrentWorkingView] * &AllPastWeeks * [Sales Domain].[Customer Group] *  [Sales Domain].[SalesOrg_DC] * [Item].[L6].[154] * [Item].[L2]) on row, ({Measure.[Actual Shifted]})  on column ;\"\n",
    "_calendar_df = \"Select ([Version].[Version Name].[CurrentWorkingView] * [Time].[Week] * [Sales Domain].[Country] ) on row, ({Measure.[Holiday Type], Measure.[Is Holiday]}) on column;\"\n",
    "_temp_planitem_df = \"Select ([Item].[L2] * [Item].[Item].filter(#.[Is New Item] != true).relatedmembers([Planning Item])* [Item].[L6].[154]);\"\n",
    "_current_week_df = \"Select (&CurrentWeek.element(0) * [Version].[Version Name]);\"\n",
    "_imputed_price_promo_df = \"Select ([Version].[Version Name].[CurrentWorkingView] * [Time].[Week] * [Sales Domain].[Customer Group] * [Item].[Planning Item]* [Item].[L6].[154] * [Sales Domain].[SalesOrg_DC]* Item.[L2]) on row,  ({Measure.[IRI Distribution Imputed], Measure.[IRI Display Only Price Imputed (lbs)], Measure.[IRI Feature Only Price Imputed (lbs)], Measure.[IRI Feature and Display Price Imputed (lbs)], Measure.[IRI Price Reduction Imputed (lbs)], Measure.[IRI Std Price Imputed (lbs)]}) on column;\"\n",
    "_time_df = \"select ([Time].[Day] * [Time].[Week]);\"\n",
    "_compass_ppg = \"Select ([Version].[Version Name].[CurrentWorkingView] * [Time].[Week] * [Sales Domain].[Customer Group] * [Item].[L2]* [Item].[L6].[154] * [Sales Domain].[SalesOrg_DC]) on row,  ({Measure.[Compass Display Only Price Imputed (EA)], Measure.[Compass Feature Only Price Imputed (EA)], Measure.[Compass Feature and Display Price Imputed (EA)], Measure.[Compass Price Reduction Imputed (EA)], Measure.[Compass Std Price Imputed (EA)],Measure.[US Compass Dist Imputed]}) on column;\"\n",
    "_uom_df = \"Select ([UOM].[UOM] * [Version].[Version Name].[CurrentWorkingView] * [Item].[Planning Item] ) on row,  ({Measure.[UOM Conversion]}) on column;\"\n",
    "_weeklist_bt_df = \"Select ([Version].[Version Name].[CurrentWorkingView] * [Time].[Week] ) on row,  ({Measure.[Week Flag]}) on column;\"\n",
    "\n",
    "\n",
    "# Initialize the O9DataLake with the input parameters and dataframes\n",
    "# Data can be accessed with O9DataLake.get(<Input Name>)\n",
    "# Overwritten values will not be reflected in the O9DataLake after initialization\n",
    "\n",
    "from o9_common_utils.O9DataLake import O9DataLake, ResourceType, DataSource\n",
    "O9DataLake.register(\"shipment_df\",DataSource.LS, ResourceType.IBPL, _shipment_df)\n",
    "O9DataLake.register(\"calendar_df\",DataSource.LS, ResourceType.IBPL, _calendar_df)\n",
    "O9DataLake.register(\"temp_planitem_df\",DataSource.LS, ResourceType.IBPL, _temp_planitem_df)\n",
    "O9DataLake.register(\"current_week_df\",DataSource.LS, ResourceType.IBPL, _current_week_df)\n",
    "O9DataLake.register(\"imputed_price_promo_df\",DataSource.LS, ResourceType.IBPL, _imputed_price_promo_df)\n",
    "O9DataLake.register(\"time_df\",DataSource.LS, ResourceType.IBPL, _time_df)\n",
    "O9DataLake.register(\"compass_ppg\",DataSource.LS, ResourceType.IBPL, _compass_ppg)\n",
    "O9DataLake.register(\"uom_df\",DataSource.LS, ResourceType.IBPL, _uom_df)\n",
    "O9DataLake.register(\"weeklist_bt_df\",DataSource.LS, ResourceType.IBPL, _weeklist_bt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "842c2daf-d4a1-4c81-beba-eb8ff8ddc670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-07 22:22:57,928 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:00,721 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:00,968 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:02,506 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:02,681 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:02,856 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:03,461 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:03,644 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:41,140 : aabhaschandra_6602 :Got invalid status code [500] at WebApi url [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery], please check url\n",
      "2022-10-07 22:23:41,142 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n",
      "2022-10-07 22:23:42,351 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/tenants/6602/batchjobs/fileDownloadBySelectQuery]\n",
      "2022-10-07 22:23:47,247 : aabhaschandra_6602 :attempt[2] command ['hdfs', 'dfs', '-ls', '/mnt/resource/settings.json'] failed\n",
      "2022-10-07 22:23:47,250 : aabhaschandra_6602 :[Errno 2] No such file or directory: 'hdfs': 'hdfs'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/genieaz_prophesyint/lib/python3.7/site-packages/o9_hdfs_utils/O9HdfsUtils.py\", line 40, in _exec_hdfs_command\n",
      "    output = subprocess.check_output(command_arr, shell=self.shell, stderr=subprocess.STDOUT)\n",
      "  File \"/opt/conda/envs/genieaz_prophesyint/lib/python3.7/subprocess.py\", line 411, in check_output\n",
      "    **kwargs).stdout\n",
      "  File \"/opt/conda/envs/genieaz_prophesyint/lib/python3.7/subprocess.py\", line 488, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"/opt/conda/envs/genieaz_prophesyint/lib/python3.7/subprocess.py\", line 800, in __init__\n",
      "    restore_signals, start_new_session)\n",
      "  File \"/opt/conda/envs/genieaz_prophesyint/lib/python3.7/subprocess.py\", line 1551, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'hdfs': 'hdfs'\n",
      "2022-10-07 22:23:47,254 : aabhaschandra_6602 :exhausted 2 retries\n",
      "2022-10-07 22:23:47,264 : aabhaschandra_6602 :IbplClient: WebApi Request Url = [https://mygenieazure.o9solutions.com/api/ibplquery/6602/ExecuteCompactJsonQuery]\n"
     ]
    }
   ],
   "source": [
    "shipment_df = O9DataLake.get(\"shipment_df\")\n",
    "calendar_df = O9DataLake.get(\"calendar_df\")\n",
    "temp_planitem_df = O9DataLake.get(\"temp_planitem_df\")\n",
    "current_week_df = O9DataLake.get(\"current_week_df\")\n",
    "imputed_price_promo_df = O9DataLake.get(\"imputed_price_promo_df\")\n",
    "time_df = O9DataLake.get(\"time_df\")\n",
    "compass_ppg = O9DataLake.get(\"compass_ppg\")\n",
    "uom_df = O9DataLake.get(\"uom_df\")\n",
    "weeklist_bt_df = O9DataLake.get(\"weeklist_bt_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a42780-75e9-4f64-8a2b-d2d7b0fdcd78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-93fbb0d8bb84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import logging\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from o9_common_utils.O9DataLake import O9DataLake\n",
    "from functools import reduce\n",
    "import io\n",
    "import os, uuid, sys\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shap\n",
    "\n",
    "logger = logging.getLogger(\"o9_logger\")\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "try:\n",
    "    import pickle5 as picl\n",
    "except:\n",
    "    logger.debug(\"pickle5 Not available\")\n",
    "\n",
    "### Data Import ###\n",
    "# Getting data from local directory if environment is local\n",
    "current_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "if current_path.split(\":\")[0] in [\"c\", \"C\", \"d\", \"D\"]:\n",
    "    print('local environment')\n",
    "    data_subpath = '\\\\package\\\\o9helpers\\\\o9_data_folder'\n",
    "    names = os.listdir(path=current_path + data_subpath)\n",
    "    for name in names:\n",
    "        exec(name + \" = pd.read_csv(r'\" + current_path + data_subpath + \"\\\\\" + name + \"\\\\000')\")\n",
    "\n",
    "else:\n",
    "    logger = logging.getLogger(\"o9_logger\")\n",
    "    logger.debug('tenant environment')\n",
    "    tenant_name = [i for i in sys.path if i.find('site-packages')!=-1][0].split('/')[4].replace('_','/')\n",
    "    logger.debug(\"Current Tenant name == \"+ tenant_name)\n",
    "\n",
    "logger.debug(\"shipment_df : {}\".format(shipment_df.shape))\n",
    "logger.debug(\"calendar_df : {}\".format(calendar_df.shape))\n",
    "\n",
    "shipment_df = shipment_df.sort_values(\n",
    "    ['Sales Domain.[Customer Group]', 'Sales Domain.[SalesOrg_DC]', 'Item.[L2]', 'Time.[Week]'])\n",
    "calendar_df = calendar_df.sort_values(['Time.[Week]'])\n",
    "time_df = time_df.sort_values(['Time.[Week]'])\n",
    "\n",
    "### Getting version and category\n",
    "current_version = shipment_df['Version.[Version Name]'].unique()[0]\n",
    "cat = shipment_df['Item.[L6]'].unique()[0]\n",
    "\n",
    "logger.debug(\"current_version - {}\".format(current_version))\n",
    "logger.debug(\"cat - {}\".format(cat))\n",
    "\n",
    "### Column handling\n",
    "### Renaming cols\n",
    "shipment_df_rename = {\n",
    "    'Time.[Week]': 'WEEK_NUM',\n",
    "    'Sales Domain.[Customer Group]': 'Planning_Entity',\n",
    "    'Sales Domain.[SalesOrg_DC]': 'SalesOrg',\n",
    "    'Item.[L2]': 'PPG',\n",
    "    'Actual Shifted': 'Actual'}\n",
    "\n",
    "uom_df_rename = {\n",
    "    'UOM.[UOM]': 'UOM',\n",
    "    'Item.[Planning Item]': 'BASECODE'\n",
    "}\n",
    "\n",
    "compass_ppg_rename = {\n",
    "    'Initiative.[Initiative]': 'Initiative',\n",
    "    'Time.[Week]': 'WEEK_NUM',\n",
    "    'Sales Domain.[Customer Group]': 'Planning_Entity',\n",
    "    'Sales Domain.[SalesOrg_DC]': 'SalesOrg',\n",
    "    'Item.[L2]': 'PPG',\n",
    "    'Compass Display Only Price Imputed (EA)': 'Display Only Price',\n",
    "    'Compass Feature Only Price Imputed (EA)': 'Feature Only Price',\n",
    "    'Compass Feature and Display Price Imputed (EA)': 'Feature and Display Price',\n",
    "    'Compass Price Reduction Imputed (EA)': 'Price Reduction Price',\n",
    "    'Compass Std Price Imputed (EA)': 'Standard Price',\n",
    "    'US Compass Dist Imputed': 'Distribution'\n",
    "}\n",
    "\n",
    "imputed_price_promo_df_rename = {\n",
    "    'Time.[Week]': 'WEEK_NUM',\n",
    "    'Sales Domain.[Customer Group]': 'Planning_Entity',\n",
    "    'Sales Domain.[SalesOrg_DC]': 'SalesOrg',\n",
    "    'Item.[L2]': 'PPG',\n",
    "    'Item.[Planning Item]': 'BASECODE',\n",
    "    'Initiative.[Initiative]': 'Initiative',\n",
    "    'IRI Display Only Price Imputed (lbs)': 'Display Only Price',\n",
    "    'IRI Feature Only Price Imputed (lbs)': 'Feature Only Price',\n",
    "    'IRI Feature and Display Price Imputed (lbs)': 'Feature and Display Price',\n",
    "    'IRI Price Reduction Imputed (lbs)': 'Price Reduction Price',\n",
    "    'IRI Std Price Imputed (lbs)': 'Standard Price',\n",
    "    'IRI Distribution Imputed': 'Distribution'}\n",
    "\n",
    "temp_compass_df_rename = {\n",
    "    'Time.[Week]': 'WEEK_NUM',\n",
    "    'Sales Domain.[Customer Group]': 'Planning_Entity',\n",
    "    'Sales Domain.[SalesOrg_DC]': 'SalesOrg',\n",
    "    \"Item.[L2]\": \"PPG\",\n",
    "    'Initiative.[Initiative]': 'Initiative',\n",
    "    'Standard Price Compass EA 445': 'Standard Price'}\n",
    "\n",
    "temp_planitem_df_rename = {'Item.[Planning Item]': 'BASECODE', \"Item.[L2]\": \"PPG\"}\n",
    "         \n",
    "                                                                                              \n",
    "\n",
    "time_df_rename = {'Time.[Week]': 'WEEK_NUM'}\n",
    "\n",
    "shipment_df.rename(columns=shipment_df_rename, inplace=True)\n",
    "imputed_price_promo_df.rename(columns=imputed_price_promo_df_rename, inplace=True)\n",
    "temp_planitem_df.rename(columns=temp_planitem_df_rename, inplace=True)\n",
    "calendar_df.rename(columns=time_df_rename, inplace=True)\n",
    "time_df.rename(columns=time_df_rename, inplace=True)\n",
    "compass_ppg.rename(columns=compass_ppg_rename, inplace=True)\n",
    "uom_df.rename(columns=uom_df_rename, inplace=True)\n",
    "                                                  \n",
    "\n",
    "                                              \n",
    "                                            \n",
    "                                 \n",
    "                               \n",
    "\n",
    "# Dropping cols\n",
    "shipment_df_drop = ['Version.[Version Name]', 'Item.[L6]']\n",
    "imputed_price_promo_df_drop = ['Version.[Version Name]', 'Item.[L6]']\n",
    "temp_planitem_df_drop = ['Item.[L6]']\n",
    "calendar_df_drop = ['Version.[Version Name]']\n",
    "time_df_drop = ['Version.[Version Name]']\n",
    "compass_ppg_drop = ['Version.[Version Name]', 'Item.[L6]']\n",
    "uom_df_drop = ['Version.[Version Name]']\n",
    "                                        \n",
    "\n",
    "shipment_df.drop(shipment_df_drop, axis=1, inplace=True)\n",
    "imputed_price_promo_df.drop(imputed_price_promo_df_drop, axis=1, inplace=True)\n",
    "temp_planitem_df.drop(temp_planitem_df_drop, axis=1, inplace=True)\n",
    "calendar_df.drop(calendar_df_drop, axis=1, inplace=True)\n",
    "uom_df.drop(uom_df_drop, axis=1, inplace=True)\n",
    "                                                    \n",
    "compass_ppg.drop(compass_ppg_drop, axis=1, inplace=True)\n",
    "\n",
    "# Week format preprocessing\n",
    "for df in [shipment_df, compass_ppg,\n",
    "           imputed_price_promo_df, time_df, calendar_df]:\n",
    "    df['WEEK_NUM'] = df['WEEK_NUM'].astype(str)\n",
    "    df['WEEK_NUM'] = df['WEEK_NUM'].str.replace('-W', '')\n",
    "\n",
    "# time_df preprocessing to extract month\n",
    "time_df['Time.[Day]'] = pd.to_datetime(time_df['Time.[Day]'])\n",
    "time_df = time_df.sort_values(['WEEK_NUM','Time.[Day]'],ascending=True)\n",
    "time_df = time_df.groupby(['WEEK_NUM'])['Time.[Day]'].first().reset_index()\n",
    "time_df['Month'] = time_df['Time.[Day]'].dt.month\n",
    "time_df.drop('Time.[Day]',axis=1,inplace=True)\n",
    "time_df['Year'] = time_df['WEEK_NUM'].apply(lambda x: x[:-2])\n",
    "time_df['Week'] = time_df['WEEK_NUM'].apply(lambda x: x[-2:])\n",
    "\n",
    "### Defining and setting dtypes\n",
    "shipment_df_dtypes = {\n",
    "    \"WEEK_NUM\": np.int64,\n",
    "    \"Planning_Entity\": \"str\",\n",
    "    \"SalesOrg\": \"str\",\n",
    "    \"PPG\": \"str\",\n",
    "    \"Actual\": \"float\"}\n",
    "\n",
    "imputed_price_promo_df_dtypes = {\n",
    "    \"WEEK_NUM\": np.int64,\n",
    "    \"Planning_Entity\": \"str\",\n",
    "    \"SalesOrg\": \"str\",\n",
    "    \"PPG\": \"str\",\n",
    "    \"BASECODE\": np.int64}\n",
    "\n",
    "temp_planitem_df_dtypes = {\n",
    "    \"PPG\": \"str\",\n",
    "    \"BASECODE\": np.int64\n",
    "}\n",
    "\n",
    "calendar_df_dtypes = {\n",
    "    \"WEEK_NUM\": np.int64\n",
    "}\n",
    "\n",
    "time_df_types = {\n",
    "    \"WEEK_NUM\": np.int64,\n",
    "    \"Year\": np.int64,\n",
    "    \"Week\": np.int64,\n",
    "    \"Month\": np.int64\n",
    "}\n",
    "\n",
    "uom_df_types = {\n",
    "    \"BASECODE\": np.int64,\n",
    "}\n",
    "\n",
    "compass_ppg_types = {\n",
    "    \"PPG\": \"str\",\n",
    "    \"WEEK_NUM\": np.int64,\n",
    "    \"SalesOrg\": \"str\",\n",
    "    \"Planning_Entity\": \"str\",\n",
    "}\n",
    "\n",
    "                \n",
    "                         \n",
    "                      \n",
    "                              \n",
    " \n",
    "\n",
    "shipment_df = shipment_df.astype(shipment_df_dtypes)\n",
    "imputed_price_promo_df = imputed_price_promo_df.astype(imputed_price_promo_df_dtypes)\n",
    "temp_planitem_df = temp_planitem_df.astype(temp_planitem_df_dtypes)\n",
    "time_df = time_df.astype(time_df_types)\n",
    "calendar_df = calendar_df.astype(calendar_df_dtypes)\n",
    "uom_df = uom_df.astype(uom_df_types)\n",
    "compass_ppg = compass_ppg.astype(compass_ppg_types)\n",
    "                                    \n",
    "\n",
    "                                                                             \n",
    "                                                                                                                                                                \n",
    "    \n",
    "                                                 \n",
    "    \n",
    "                              \n",
    "                                                                          \n",
    "                                     \n",
    "    \n",
    "                               \n",
    "                                                                                                                                                  \n",
    "                                                        \n",
    "\n",
    "                                 \n",
    "                           \n",
    "                                                                      \n",
    "                                 \n",
    "                                   \n",
    "    \n",
    "    \n",
    "                                            \n",
    "    \n",
    "\n",
    "                                               \n",
    "                               \n",
    "    \n",
    "                                                                          \n",
    "                                                                                       \n",
    "                                                                                                                                                 \n",
    "    \n",
    "                                            \n",
    "                                    \n",
    "                                                                                                                        \n",
    "                                        \n",
    "    \n",
    "                                                         \n",
    "                                                                             \n",
    "\n",
    "                           \n",
    "                                                                  \n",
    "                                                                  \n",
    "                                                                  \n",
    "        \n",
    "                                        \n",
    "                                                \n",
    "    \n",
    "                          \n",
    "                                                                                \n",
    "\n",
    "        \n",
    "                                                      \n",
    "             \n",
    "\n",
    "def feat_imp(model, train_x1, current_version, ship_max_week, featflag, cat):\n",
    "    feat = model.feature_importances_\n",
    "    col = train_x1.columns\n",
    "    feat_df = pd.DataFrame({'Columns': col, 'Importance': feat})\n",
    "\n",
    "    feat_data = feat_df.T\n",
    "    feat_data.columns = feat_data.iloc[0]\n",
    "    feat_data = feat_data[1:]\n",
    "    feat_data = feat_data.reset_index(drop=True)\n",
    "\n",
    "    feat_data['Holiday'] = 0\n",
    "    feat_data['Base'] = 0\n",
    "    feat_data['Promotion'] = 0\n",
    "\n",
    "    logger.debug('Initial feat_data:')\n",
    "    logger.debug(feat_data)\n",
    "\n",
    "    allocated_cols = ['Distribution', 'DisplayOnlyPrice',\n",
    "     'FeatureOnlyPrice', 'FeatureandDisplayPrice', 'PriceReductionPrice', 'StandardPrice']\n",
    "\n",
    "    for i in feat_data.columns:\n",
    "        if str(i).startswith(\"holiday\"):\n",
    "            feat_data['Holiday'] = feat_data[\"Holiday\"] + feat_data[i]\n",
    "            allocated_cols.append(i)\n",
    "        #if str(i).startswith(\"US\"):\n",
    "            #feat_data['Base'] = feat_data[\"Base\"] + feat_data[i]\n",
    "            #allocated_cols.append(i)            \n",
    "        #if len(str(i))==15:\n",
    "            #feat_data['Base'] = feat_data[\"Base\"] + feat_data[i]\n",
    "            #allocated_cols.append(i)\n",
    "        if 'FLAG' in str(i):\n",
    "            feat_data['Promotion'] = feat_data[\"Promotion\"] + feat_data[i]\n",
    "            allocated_cols.append(i)\n",
    "            \n",
    "\n",
    "    feat_data['Time Trend'] = feat_data['Week'] + feat_data['Month']\n",
    "    allocated_cols.append('Week'); allocated_cols.append('Month'); \n",
    "    \n",
    "    feat_data['Seasonality'] = feat_data['mnth_cos'] + feat_data['mnth_sin'] + feat_data['week_cos'] + feat_data[\n",
    "        'week_sin']\n",
    "    allocated_cols.append('mnth_cos'); allocated_cols.append('mnth_sin'); \n",
    "    allocated_cols.append('week_cos'); allocated_cols.append('week_sin'); \n",
    "\n",
    "    created_cols = ['Time Trend', 'Seasonality','Promotion','Holiday','Base']    \n",
    "    unallocated_cols = list(set(feat_data.columns) - set(allocated_cols) - set(created_cols))\n",
    "\n",
    "    for i in unallocated_cols:\n",
    "        feat_data['Base'] = feat_data[\"Base\"] + feat_data[i]\n",
    "\n",
    "    if featflag:\n",
    "        feat_data = feat_data[\n",
    "            ['Holiday', 'Base', 'Promotion', 'Time Trend', 'Seasonality', 'Distribution', 'DisplayOnlyPrice',\n",
    "             'FeatureOnlyPrice', 'FeatureandDisplayPrice', 'PriceReductionPrice', 'StandardPrice']]\n",
    "        feat_data.rename(columns={'DisplayOnlyPrice': 'Display Only Price',\n",
    "                                  'FeatureOnlyPrice': 'Feature Only Price',\n",
    "                                  'FeatureandDisplayPrice': 'Feature and Display Price',\n",
    "                                  'PriceReductionPrice': 'Price Reduction Price', 'StandardPrice': 'Standard Price'},\n",
    "                         inplace=True)\n",
    "    else:\n",
    "        feat_data = feat_data[['Holiday', 'Base', 'Time Trend', 'Seasonality']]\n",
    "\n",
    "    feat_data['Version.[Version Name]'] = current_version\n",
    "    feat_data['Time.[Week]'] = ship_max_week\n",
    "    feat_data['Item.[L6]'] = cat\n",
    "\n",
    "    feat_data['Sales Domain.[Sales Org]'] = 'US01'\n",
    "\n",
    "    feat_data['Time.[Week]'] = feat_data['Time.[Week]'].map(lambda x: str(x)[:4] + \"-W\" + str(x)[4:])\n",
    "\n",
    "    feat_data['Item.[L6]'] = feat_data['Item.[L6]'].map(lambda x: '0' + str(x) if len(str(x)) == 2 else str(x))\n",
    "\n",
    "                                                 \n",
    "\n",
    "    feat_data = feat_data.melt(id_vars=['Version.[Version Name]', 'Time.[Week]', 'Item.[L6]','Sales Domain.[Sales Org]'],\n",
    "                               var_name='[ML Driver].[ML Driver]', value_name='Values')\n",
    "    feat_data.rename(columns={'Values': 'Driver Importance'}, inplace=True)\n",
    "\n",
    "    feat_data['Driver Importance'] = feat_data['Driver Importance'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "    if \"CatBoost\" in str(type(model)):\n",
    "        feat_data.rename(columns={'Driver Importance':'Driver Importance Model 1'}, inplace=True)\n",
    "        feat_data['Driver Importance Model 1'] = feat_data['Driver Importance Model 1'] / 100\n",
    "    if \"LGBM\" in str(type(model)):\n",
    "        feat_data.rename(columns={'Driver Importance':'Driver Importance Model 2'}, inplace=True)\n",
    "        importance_sum = feat_data['Driver Importance Model 2'].sum()\n",
    "        if importance_sum!=0:\n",
    "            feat_data['Driver Importance Model 2'] = feat_data['Driver Importance Model 2']/importance_sum\n",
    "        else:\n",
    "            feat_data['Driver Importance Model 2'] = 0\n",
    "    if \"RandomForest\" in str(type(model)):\n",
    "        feat_data.rename(columns={'Driver Importance':'Driver Importance Model 3'}, inplace=True)\n",
    "    if \"XGB\" in str(type(model)):\n",
    "        feat_data.rename(columns={'Driver Importance':'Driver Importance Model 4'}, inplace=True)\n",
    "    return feat_data\n",
    "\n",
    "def upload_ads_to_azure(source_df, azure_connection_string, destination_file_name, destination_name):\n",
    "    \"\"\"\n",
    "    Function to upload data from contents to an Azure container\n",
    "    :source_df: data frame name that needs to be transferred\n",
    "\n",
    "    :param azure_connection_string: .\n",
    "    :param destination_file_name:\n",
    "    :param destination:\n",
    "    :param azure_container_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logger.debug('Trying to uploading contents of {} to {} located in Azure container {}.'.format(\"source_df\",\n",
    "                                                                                                  destination_name + \"/ \" + destination_file_name,\n",
    "                                                                                                  destination_name))\n",
    "    try:\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(azure_connection_string)\n",
    "\n",
    "        output_par = io.BytesIO()\n",
    "        table = pa.Table.from_pandas(source_df)\n",
    "        pq.write_table(table, output_par)\n",
    "        output_par.seek(0)\n",
    "        blob_client = blob_service_client.get_blob_client(container=destination_name,\n",
    "                                                          blob=destination_file_name + \".parquet\")\n",
    "        if blob_client.exists():\n",
    "            blob_client.delete_blob()\n",
    "        blob_client.upload_blob(output_par)\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        logging.exception(\"Azure upload to {} failed.\".format(destination_name + \"/ \" + destination_file_name))\n",
    "        return False\n",
    "\n",
    "\n",
    "def upload_pickle_to_azure(source_pickle, azure_connection_string, destination_file_name, destination_name):\n",
    "    logger.debug('Trying to uploading contents of {} to {} located in Azure container {}.'.format(\"source_df\",\n",
    "                                                                                                  destination_name + \"/ \" + destination_file_name,\n",
    "                                                                                                  destination_name))\n",
    "    try:\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(azure_connection_string)\n",
    "        blob_client = blob_service_client.get_blob_client(container=destination_name, blob=destination_file_name)\n",
    "        if blob_client.exists():\n",
    "            blob_client.delete_blob()\n",
    "        output = io.BytesIO()\n",
    "        picl.dump(source_pickle, output)\n",
    "        output.seek(0)\n",
    "        blob_client.upload_blob(output)\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        logging.exception(\"Azure upload to {} failed.\".format(destination_name + \"/ \" + destination_file_name))\n",
    "\n",
    "\n",
    "def ads(shipment_df, imputed_price_promo_df, year_week_df, calendar_df, uom_df, compass_ppg):\n",
    "    #### Processing shipment data ####\n",
    "\n",
    "    # Column \"Shipment_Volume\" consists of values (0,+ve values)\n",
    "    shipment_df.loc[shipment_df['Actual'] < 0, 'Shipment_Volume'] = 0\n",
    "    shipment_df['Shipment_Volume'].fillna(shipment_df['Actual'], inplace=True)\n",
    "    logger.debug(shipment_df.shape)\n",
    "    logger.debug(\"shipment_df - Shipment_Volume column and value\")\n",
    "    logger.debug(shipment_df.head())\n",
    "\n",
    "    shipment_df = shipment_df.groupby(['PPG', 'Planning_Entity', 'WEEK_NUM', 'SalesOrg'])[\n",
    "        'Shipment_Volume'].sum().reset_index()\n",
    "    if RunType=='Forward':\n",
    "        current_week_df['Time.[Week]'] = current_week_df['Time.[Week]'].str.replace('-W', '')\n",
    "        current_week = current_week_df['Time.[Week]'].unique()[0]\n",
    "    elif RunType=='Backtest':\n",
    "        weeklist_bt_df['Time.[Week]'] = weeklist_bt_df['Time.[Week]'].str.replace('-W', '')\n",
    "        current_week = weeklist_bt_df['Time.[Week]'].unique()[0]\n",
    "    current_week = int(current_week)\n",
    "    logger.debug(\"Current week of the forecast\")\n",
    "    logger.debug(current_week)\n",
    "\n",
    "    logger.debug(\"shipment_df shape\")\n",
    "    logger.debug(shipment_df.shape)\n",
    "    # Creating ship max week after filtering\n",
    "    ship_max_idx = year_week_df[time_df['WEEK_NUM'] == current_week].index - 1\n",
    "    ship_max_week = year_week_df.loc[ship_max_idx, 'WEEK_NUM'].values[0]\n",
    "    logger.debug(\"ship_max_week\")\n",
    "    logger.debug(ship_max_week)\n",
    "\n",
    "    # creating ship min week to filter out\n",
    "    ship_min_week = shipment_df['WEEK_NUM']\n",
    "    ship_min_week = ship_min_week.min()\n",
    "    logger.debug(\"Shipment min week num\")\n",
    "    logger.debug(ship_min_week)\n",
    "\n",
    "    Train_Periods = 156\n",
    "    training_start_week_index = year_week_df[year_week_df['WEEK_NUM'] == current_week].index - Train_Periods #!! Should be current_week\n",
    "    starting_week = year_week_df.loc[training_start_week_index, 'WEEK_NUM'].values[0]\n",
    "\n",
    "    filter_week_index = year_week_df[year_week_df['WEEK_NUM'] == current_week].index - 54 #!! Should be current_week\n",
    "    filter_week = year_week_df.loc[filter_week_index, 'WEEK_NUM'].values[0]\n",
    "\n",
    "    # Creating ending_week = current_week + Number_Of_Weeks + Max number of weeks to get lead features (4 for holidays) TODO: check if necessary\n",
    "    ending_week_index = year_week_df[year_week_df['WEEK_NUM'] == current_week].index + (int(Number_Of_Weeks)) + 4 #!! Should be current_week\n",
    "    ending_week = year_week_df.loc[ending_week_index, 'WEEK_NUM'].values[0]\n",
    "\n",
    "    iri_max_week_index = time_df[time_df['WEEK_NUM'] == current_week].index - 4\n",
    "    iri_max_week = time_df.loc[iri_max_week_index, 'WEEK_NUM'].values[0]\n",
    "\n",
    "    compass_start_week_index = iri_max_week_index + 1\n",
    "    compass_start_week = time_df.loc[compass_start_week_index, 'WEEK_NUM'].values[0]\n",
    "\n",
    "    # filtering out current week shipment\n",
    "    shipment_df = shipment_df[shipment_df['WEEK_NUM'] < current_week]\n",
    "\n",
    "    # obtain the df of valid intersections to fcst in the future (intersections with data in past 54 wks)\n",
    "    map_df = shipment_df[shipment_df['WEEK_NUM'] >= filter_week][\n",
    "        ['PPG', 'Planning_Entity', 'SalesOrg']].drop_duplicates()\n",
    "    \n",
    "    # Filtering out all combinations not present in last 54 weeks\n",
    "    shipment_df = pd.merge(map_df, shipment_df, on=['PPG', 'Planning_Entity', 'SalesOrg'], how='left')\n",
    "\n",
    "    # For fcst intersections that have weeks where there is no shipment, we want to create rows with zero shipment\n",
    "    # get the weeks for which we are considering shipments\n",
    "    time_df_map = year_week_df[(year_week_df['WEEK_NUM'] >= starting_week) &\n",
    "                               (year_week_df['WEEK_NUM'] <= ship_max_week)][['WEEK_NUM']]\n",
    "    # get all fcst intersections which have occurred in the past\n",
    "    # shipment_df_map = shipment_df[['PPG', 'Planning_Entity', 'SalesOrg']].drop_duplicates()\n",
    "    map_df['key'] = 0\n",
    "    time_df_map['key'] = 0\n",
    "\n",
    "    # perform a cross join of fcst intersections and time so that  there is a week for each fcst intersection\n",
    "    # Example: WEEK_NUM x PPG            x Planning_Entity x SalesOrg   Shipment_Volume\n",
    "    #          202130     02300910021600F  US4000415         US01-20    5472\n",
    "    #          202135     02300910021600F  US4000415         US01-20    11.4\n",
    "    # becomes\n",
    "    #          WEEK_NUM x PPG            x Planning_Entity x SalesOrg   Shipment_Volume\n",
    "    #          202129     02300910021600F  US4000415         US01-20    nan    \n",
    "    #          202130     02300910021600F  US4000415         US01-20    5472\n",
    "    #          202131     02300910021600F  US4000415         US01-20    nan\n",
    "    #          202132     02300910021600F  US4000415         US01-20    nan\n",
    "    #          202133     02300910021600F  US4000415         US01-20    nan\n",
    "    #          202134     02300910021600F  US4000415         US01-20    nan\n",
    "    #          202135     02300910021600F  US4000415         US01-20    11.4\n",
    "    shipment_df_temp = pd.merge(time_df_map, map_df, on='key', how='left')\n",
    "    shipment_df_temp.drop('key', axis=1, inplace=True)\n",
    "    map_df.drop('key', axis=1, inplace=True)\n",
    "\n",
    "    # join that tmp ship df back on the original ship df. Any originally missing weeks for intersections will be NaN\n",
    "    shipment_df = pd.merge(shipment_df_temp, shipment_df, on=['PPG', 'Planning_Entity', 'WEEK_NUM', 'SalesOrg'],\n",
    "                           how='left')\n",
    "\n",
    "    # for each forecast intersection, get the first week for which there was actuals\n",
    "    first_shipments = shipment_df[shipment_df['Shipment_Volume'] > 0].groupby(['PPG', 'Planning_Entity', 'SalesOrg'])[\n",
    "        'WEEK_NUM'].first().reset_index()\n",
    "    first_shipments.rename({'WEEK_NUM': 'first_week'}, axis=1, inplace=True)\n",
    "\n",
    "    # bring that first shipment week column into our shipment df and filter to remove weeks before first shipment\n",
    "    # Example: WEEK_NUM x PPG            x Planning_Entity x SalesOrg   Shipment_Volume\n",
    "    #          202128     02300910021600F  US4000415         US01-20    nan\n",
    "    #          202129     02300910021600F  US4000415         US01-20    nan\n",
    "    #          202130     02300910021600F  US4000415         US01-20    5472\n",
    "    # where 202130 is the first shipment volume for this combination, is replaced by\n",
    "    #          WEEK_NUM x PPG            x Planning_Entity x SalesOrg   Shipment_Volume\n",
    "    #          202130     02300910021600F  US4000415         US01-20    5472    \n",
    "    # and all later weeks\n",
    "\n",
    "    # bring that first shipment week column into our shipment df and filter to remove weeks before first shipment\n",
    "    shipment_df = pd.merge(shipment_df, first_shipments, on=['PPG', 'Planning_Entity', 'SalesOrg'])\n",
    "    shipment_df = shipment_df[(shipment_df['WEEK_NUM']) >= (shipment_df['first_week'])]\n",
    "    shipment_df.drop('first_week', axis=1, inplace=True)\n",
    "\n",
    "    # Processing Sales Driver data #\n",
    "    # Creating flag for categories w/o driver data\n",
    "    FSFlag = True if imputed_price_promo_df.shape[0] == 0 else False\n",
    "    logger.debug(\"FS Flag\")\n",
    "    logger.debug(FSFlag)\n",
    "\n",
    "    # IRI Price\n",
    "    if not FSFlag:\n",
    "        logger.debug(\"Not FSFlag\")\n",
    "        # Compass Processing\n",
    "        # IRI start/end week and compass start/end week\n",
    "        compass_ppg = compass_ppg[(compass_ppg.WEEK_NUM >= compass_start_week) & (compass_ppg.WEEK_NUM <= ending_week)]\n",
    "\n",
    "        # Considering only the price columns for mapping to basecode level with temp_planitem_df (Only prices require UoM conversion)\n",
    "        compass_df = pd.merge(compass_ppg.drop('Distribution',axis=1), temp_planitem_df, on='PPG', how='left')\n",
    "        \n",
    "        uom_df = uom_df[uom_df['UOM'] == 'EA']\n",
    "        compass_df = pd.merge(compass_df, uom_df, on='BASECODE', how='left').drop_duplicates()\n",
    "        compass_df.loc[compass_df['UOM Conversion'].isnull(), 'UOM Conversion'] = 1\n",
    "\n",
    "        compass_df.rename(columns={'Compass Distribution Imputed': 'Distribution'}, inplace=True)\n",
    "\n",
    "        compass_df = (compass_df.groupby(by=['Planning_Entity', 'PPG', 'WEEK_NUM', 'SalesOrg'], observed=True)[\n",
    "                          ['Display Only Price', 'Feature Only Price',\n",
    "                           'Feature and Display Price',\n",
    "                           'Price Reduction Price',\n",
    "                           'Standard Price',\n",
    "                           'UOM Conversion'\n",
    "                           ]]\n",
    "                      .mean().reset_index())\n",
    "\n",
    "        compass_df = pd.merge(compass_df, compass_ppg[['WEEK_NUM', 'Planning_Entity', 'SalesOrg',\n",
    "                                                                  'Distribution', 'PPG']],\n",
    "                              on=['WEEK_NUM', 'Planning_Entity', 'SalesOrg', 'PPG'])\n",
    "\n",
    "\n",
    "        # Converting using conversion factor\n",
    "        compass_df['Display Only Price'] = compass_df['Display Only Price'] * compass_df['UOM Conversion']\n",
    "        compass_df['Feature Only Price'] = compass_df['Feature Only Price'] * compass_df['UOM Conversion']\n",
    "        compass_df['Feature and Display Price'] = compass_df['Feature and Display Price'] * compass_df['UOM Conversion']\n",
    "        compass_df['Price Reduction Price'] = compass_df['Price Reduction Price'] * compass_df['UOM Conversion']\n",
    "        compass_df['Standard Price'] = compass_df['Standard Price'] * compass_df['UOM Conversion']\n",
    "\n",
    "        compass_df.drop(['UOM Conversion'], axis=1, inplace=True)\n",
    "\n",
    "        # IRI Processing\n",
    "        # Price/Promotion data preprocessing- (not necessary for TDP data) #\n",
    "        # imputed_price_promo_df = imputed_price_promo_df[\n",
    "            # [col for col in imputed_price_promo_df.columns if 'Compass' not in col]]\n",
    "        imputed_price_promo_df = imputed_price_promo_df[(imputed_price_promo_df.WEEK_NUM >= starting_week) &\n",
    "                                                        (imputed_price_promo_df.WEEK_NUM <= iri_max_week)]\n",
    "        \n",
    "        imputed_price_promo_df['IRI_flag'] = np.where(imputed_price_promo_df['Standard Price']>0, 1, 0)\n",
    "\n",
    "        # IRI_flag is next aggregated over all the basecodes under each ppg\n",
    "        # Taking a single PPG and WEEK_NUM for example\n",
    "        # Example: WEEK_NUM x BASECODE    x Planning_Entity x SalesOrg x PPG             .... x IRI_flag\n",
    "        #          201901     430000294500  US3000059         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294500  US4000150         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294500  US4000071         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294500  US4000151         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294500  US2000013         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294500  US3000030         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294500  US4000153         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294800  US2000013         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294900  US2000013         US01-20    023011300312XF8        1\n",
    "        #          201901     430000294900  US3000030         US01-20    023011300312XF8        1\n",
    "        #          201901     430000295100  US4000071         US01-20    023011300312XF8        1\n",
    "        #          201901     430000295100  US3000030         US01-20    023011300312XF8        1\n",
    "        #          201901     430007086500  US2000013         US01-20    023011300312XF8        1\n",
    "        #          201901     430007086500  US3000059         US01-20    023011300312XF8        1\n",
    "        #          201901     430007086500  US4000153         US01-20    023011300312XF8        1\n",
    "        #          201901     430007086500  US3000030         US01-20    023011300312XF8        1\n",
    "\n",
    "        agg_by_dict = {'Display Only Price': 'mean', \n",
    "                       'Feature Only Price': 'mean',\n",
    "                       'Feature and Display Price': 'mean',\n",
    "                       'Price Reduction Price': 'mean',\n",
    "                       'Standard Price': 'mean', \n",
    "                       'Distribution': 'mean',\n",
    "                       'IRI_flag': 'sum'}\n",
    "        imputed_price_promo_df = (imputed_price_promo_df.groupby(['WEEK_NUM', 'PPG', 'Planning_Entity', 'SalesOrg'])\n",
    "                                  ['Standard Price', 'Display Only Price', 'Feature Only Price',\n",
    "                                      'Feature and Display Price', 'Price Reduction Price', 'Distribution','IRI_flag']\n",
    "                                  .agg(agg_by_dict).reset_index())\n",
    "\n",
    "        # After aggregation, \n",
    "        # Example: WEEK_NUM x Planning_Entity x SalesOrg x PPG             .... x IRI_flag\n",
    "        #          201901     US3000059         US01-20    023011300312XF8        2\n",
    "        #          201901     US4000150         US01-20    023011300312XF8        1\n",
    "        #          201901     US4000071         US01-20    023011300312XF8        2\n",
    "        #          201901     US4000151         US01-20    023011300312XF8        1\n",
    "        #          201901     US2000013         US01-20    023011300312XF8        4\n",
    "        #          201901     US3000030         US01-20    023011300312XF8        4\n",
    "        #          201901     US4000153         US01-20    023011300312XF8        2\n",
    "        \n",
    "                                                                                  \n",
    "                                                                                      \n",
    "\n",
    "        #Setting up a mapping dataframe using IRI_flag from imputed_price_promo_df  \n",
    "        #This helps propagate IRI_flag to the Compass (future) part of sales_driver_df\n",
    "        sdmap_df = imputed_price_promo_df[['PPG', 'Planning_Entity', 'SalesOrg', 'IRI_flag']].drop_duplicates()\n",
    "        imputed_price_promo_df.drop('IRI_flag',axis=1,inplace=True) \n",
    "\n",
    "        logger.debug('imputed_price_promo_df shape')\n",
    "        logger.debug(imputed_price_promo_df.shape)\n",
    "        logger.debug('imputed_price_promo_df columns')\n",
    "        logger.debug(imputed_price_promo_df.columns)\n",
    "\n",
    "                                                                                                      \n",
    "\n",
    "        sales_driver_df = pd.concat([imputed_price_promo_df, compass_df], axis=0) #!!Is this sufficient?\n",
    "        \n",
    "        sales_driver_df = pd.merge(sdmap_df, sales_driver_df, on=['PPG', 'Planning_Entity', 'SalesOrg'], how='left')\n",
    "\n",
    "        logger.debug(\"Sales driver shape\")\n",
    "        logger.debug(sales_driver_df.shape)\n",
    "        logger.debug(\"Sales driver describe\")\n",
    "        logger.debug(sales_driver_df.describe())\n",
    "\n",
    "    else:\n",
    "        logger.debug(\"No sales driver processing possible\")\n",
    "\n",
    "    # Merging shipment with sales drivers #\n",
    "    # Creating all combinations of basecode, planning entity & salesorg that are present in last 54 weeks\n",
    "    time_df_map = year_week_df[(year_week_df['WEEK_NUM'] >= current_week) &\n",
    "                               (year_week_df['WEEK_NUM'] <= ending_week)][['WEEK_NUM']] #!!Brings in only WEEK_NUM, which is correct\n",
    "\n",
    "    # Cross join - Each combination is being mapped to each week in the entire 156 weeks + next 114 weeks\n",
    "    map_df['key'] = 1\n",
    "    time_df_map['key'] = 1\n",
    "    future_records = pd.merge(time_df_map, map_df, on='key')\n",
    "    future_records.drop(columns='key', inplace=True)\n",
    "\n",
    "    # Stack the shipment history with the future intersections to forecast\n",
    "    main_df = pd.concat([shipment_df, future_records], axis=0)\n",
    "\n",
    "    main_df = main_df[['WEEK_NUM', 'PPG', 'Planning_Entity', 'SalesOrg', 'Shipment_Volume']]\n",
    "    logger.debug(\"Filtering nulls weeks before starting the shipment\")\n",
    "    logger.debug(main_df.shape)\n",
    "    logger.debug(main_df.head())\n",
    "    \n",
    "                              \n",
    "                                                                       \n",
    "                                                                               \n",
    "    \n",
    "                                             \n",
    "                                          \n",
    "    \n",
    "                                                                                  \n",
    "                                                                                      \n",
    "\n",
    "    # Merging shipment with sales drivers #\n",
    "    if not FSFlag:\n",
    "        logger.debug(\"not FSFlag - 1\")\n",
    "\n",
    "        main_df = pd.merge(main_df, sales_driver_df, on=['WEEK_NUM', 'PPG', 'Planning_Entity', 'SalesOrg'],\n",
    "                           how='left').drop_duplicates()\n",
    "\n",
    "        # Example\n",
    "        # WEEK_NUM x PPG              x Planning_Entity x SalesOrg    Shipment_Volume    x IRI_flag    \n",
    "        # 201927     023009100216JE8    US4000576          US01-20    510                  3            \n",
    "        # 201928     023009100216JE8    US4000576          US01-20    nan                  3            \n",
    "        # cont..\n",
    "        # Display Only Price x Feature Only Price x Feature and Display Price x Price Reduction Price x Standard Price x Distribution\n",
    "        # 7.346199989           7.346199989            7.346199989                    7.036043326                7.346199989         84.4221\n",
    "        # 7.333030065           7.333030065            7.333030065                    5.380936623                7.333030065         83.01346667\n",
    "\n",
    "        for col in ['Display Only Price', 'Feature Only Price', 'Feature and Display Price', 'Price Reduction Price']:\n",
    "            main_df[col + ' ratio'] = (main_df['Standard Price'].round(2) - main_df[col].round(2)) / main_df[\n",
    "                'Standard Price'].round(2)\n",
    "            main_df[col] = main_df[col + ' ratio'].apply(lambda x: abs(100 * x)).round(2)\n",
    "            main_df[col].fillna(0, inplace=True)\n",
    "            main_df.drop([col + ' ratio'], axis=1, inplace=True)\n",
    "\n",
    "        # After transformation becomes\n",
    "        # WEEK_NUM x PPG              x Planning_Entity x SalesOrg    Shipment_Volume    x IRI_flag    \n",
    "        # 201927     023009100216JE8    US4000576          US01-20    510                  3            \n",
    "        # 201928     023009100216JE8    US4000576          US01-20    nan                  3            \n",
    "        # cont..\n",
    "        # Display Only Price x Feature Only Price x Feature and Display Price x Price Reduction Price x Standard Price x Distribution\n",
    "        # 0                       0                    0                            4.22                    7.346199989         84.4221\n",
    "        # 0                       0                    0                            26.6                    7.333030065         83.01346667    \n",
    "\n",
    "        # Promotion Measures- If the promo price feature is > 0, then the value is flagged as 1 (promotion flag)\n",
    "        main_df['DISPLAY_ONLY_PROMO_FLAG'] = np.where(main_df['Display Only Price'] > 0, 1, 0)\n",
    "        main_df['FEATURE_DISPLAY_PROMO_FLAG'] = np.where(main_df['Feature and Display Price'] > 0, 1, 0)\n",
    "        main_df['FEATURE_ONLY_PROMO_FLAG'] = np.where(main_df['Feature Only Price'] > 0, 1, 0)\n",
    "        main_df['PRICE_REDUCTIONS_ONLY_PROMO_FLAG'] = np.where(main_df['Price Reduction Price'] > 0, 1, 0)\n",
    "\n",
    "        main_df = main_df.sort_values(['WEEK_NUM'])\n",
    "\n",
    "        # Creating lead1 and lead2 features for Promotions\n",
    "        # Note- lead1 corresponds to shift(-2) due to KHC convention of treating current week as lag -1\n",
    "        promo_col = [col for col in main_df if col.endswith('FLAG')]\n",
    "        # Eg. For a combination, row of W35, Lead1 will have the flag value of week 37 if there is a shift of -2 #!!Change to lead1\n",
    "        for col in promo_col:\n",
    "            main_df[col + '_lead2'] = main_df.groupby(['PPG', 'Planning_Entity', 'SalesOrg'])[col].shift(-2)\n",
    "            main_df[col + '_lead3'] = main_df.groupby(['PPG', 'Planning_Entity', 'SalesOrg'])[col].shift(-3)\n",
    "\n",
    "    else:\n",
    "        logger.debug(\"FSFlag is true\")\n",
    "        # Setting IRI_flag\n",
    "        main_df['IRI_flag'] = 0\n",
    "        logger.debug(\"main_df shape after setting IRI_flag\")\n",
    "        logger.debug(main_df.shape)\n",
    "        logger.debug(\"main_df head\")\n",
    "        logger.debug(main_df.head())\n",
    "\n",
    "    # Extraction of Holiday Calendar #\n",
    "    calendar_df = calendar_df[calendar_df['Sales Domain.[Country]'] == 'US']\n",
    "\n",
    "    # Creating a matrix of all holidays as column and then allocation 'no of holidays' for a specific week\n",
    "    calendar_df = calendar_df.pivot_table(index=['WEEK_NUM'], columns='Holiday Type', values='Is Holiday',\n",
    "                                          aggfunc='mean', fill_value=0).reset_index()\n",
    "    calendar_df = calendar_df.rename_axis(None, axis=1)\n",
    "\n",
    "    # Renaming holiday columns\n",
    "    for i in calendar_df.columns:\n",
    "        if 'WEEK' not in i:\n",
    "            calendar_df.rename(columns={i: 'holiday_' + i}, inplace=True)\n",
    "    logger.debug(\"Holiday shape\")\n",
    "    logger.debug(calendar_df.shape)\n",
    "\n",
    "    main_df = pd.merge(main_df, calendar_df, on='WEEK_NUM', how='left').drop_duplicates()\n",
    "\n",
    "    # Creating lead1 and lead2 features for Holiday\n",
    "    holiday_col = [col for col in main_df if col.startswith('holiday')]\n",
    "\n",
    "    # Eg. For a combination, row of W35, Lead1 will have the flag value of week 37 if there is a shift of -2\n",
    "    # Note- lead1 corresponds to shift(-2) due to KHC convention of treating current week as lag -1\n",
    "    main_df = main_df.sort_values(['WEEK_NUM'])\n",
    "    for col in holiday_col:\n",
    "        main_df[col + '_lead2'] = main_df.groupby(['PPG', 'Planning_Entity', 'SalesOrg'])[col].shift(-2) #!! Should be calling it lead1\n",
    "        main_df[col + '_lead3'] = main_df.groupby(['PPG', 'Planning_Entity', 'SalesOrg'])[col].shift(-3)\n",
    "        main_df[col + '_lead4'] = main_df.groupby(['PPG', 'Planning_Entity', 'SalesOrg'])[col].shift(-4)\n",
    "\n",
    "    logger.debug(\"main df shape after holiday merging\")\n",
    "    logger.debug(main_df.shape)\n",
    "\n",
    "    # Creating Week, Year and Month columns- Week and Month are used as features #\n",
    "    main_df = pd.merge(main_df, year_week_df, on='WEEK_NUM', how='left')\n",
    "\n",
    "    # Creating cyclic (trigonometric) features #\n",
    "    # Creating sin and cos month cyclic values\n",
    "    main_df['mnth_sin'] = np.sin((main_df.Month - 1) * (2. * np.pi / 12))\n",
    "    main_df['mnth_cos'] = np.cos((main_df.Month - 1) * (2. * np.pi / 12))\n",
    "    # creating sin and cos weekly cyclic values\n",
    "    main_df['week_sin'] = np.sin((main_df.Week - 1) * (2. * np.pi / 52))\n",
    "    main_df['week_cos'] = np.cos((main_df.Week - 1) * (2. * np.pi / 52))\n",
    "\n",
    "    # Example\n",
    "    # WEEK_NUM x PPG              x Planning_Entity    x SalesOrg x Shipment_Volume x Month x Year x Week x mnth_sin x mnth_cos x week_sin    x week_cos\n",
    "    # 201905     0230113003123QH    US4000071          US01-20      0                   1       2019      5         0            1           0.464723172     0.885456026\n",
    "\n",
    "    logger.debug(\"Shape of Main data before splitting into ML and MLL\")\n",
    "    logger.debug(main_df.shape)\n",
    "\n",
    "    # Partitioning for driver (ML) and driverless (MLL) parts of the ADS #\n",
    "    # data for ML model\n",
    "    ml_df = main_df[main_df['IRI_flag'] >= 1]\n",
    "    ml_df.drop(['IRI_flag'], axis=1, inplace=True)\n",
    "\n",
    "    logger.debug(\"Shape of ml_df\")\n",
    "    logger.debug(ml_df.shape)\n",
    "\n",
    "    # data for MLL model\n",
    "    mll_df = main_df[main_df['IRI_flag'].fillna(0) == 0]\n",
    "    mll_df.drop(['IRI_flag'], axis=1, inplace=True)\n",
    "\n",
    "    logger.debug(\"Shape of mll_df\")\n",
    "    logger.debug(mll_df.shape)\n",
    "\n",
    "    # removing space and punctuations from column names\n",
    "    ml_df = ml_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    mll_df = mll_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "    return ml_df, mll_df, starting_week, ship_max_week, ship_min_week\n",
    "\n",
    "\n",
    "def model_results(forecast_list, current_version):\n",
    "    # Generating o9 output #\n",
    "    df_results = pd.concat(forecast_list, ignore_index=True)\n",
    "\n",
    "    # Shape of result before EPM merging\n",
    "    logger.debug(\"Shape of result before EPM merging\")\n",
    "    logger.debug(df_results.shape)\n",
    "\n",
    "    # converting to o9 format and adding EPM column\n",
    "    df_results = df_results[['Time.[Week]', 'PPG', 'Planning_Entity', 'SalesOrg', 'Predicted']]\n",
    "    df_results['PPG'] = df_results['PPG'].astype(str)\n",
    "    df_results['Planning_Entity'] = df_results['Planning_Entity'].astype(str)\n",
    "    df_results['SalesOrg'] = df_results['SalesOrg'].astype(str)\n",
    "    df_results.rename(columns={'PPG': 'Item.[L2]'}, inplace=True)\n",
    "    df_results.rename(columns={'Planning_Entity': 'Sales Domain.[Customer Group]'}, inplace=True)\n",
    "    df_results['Version.[Version Name]'] = current_version\n",
    "    df_results['Time.[Week]'] = df_results['Time.[Week]'].map(lambda x: str(x)[:4] + \"-W\" + str(x)[4:])\n",
    "    df_results = df_results[\n",
    "        ['Version.[Version Name]', 'Item.[L2]', 'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg',\n",
    "         'Predicted']]\n",
    "    df_results.loc[df_results['Predicted'] <= 0, 'Predicted'] = 0\n",
    "\n",
    "    # Shape of result\n",
    "    logger.debug(\"Shape of result after renaming\")\n",
    "    logger.debug(df_results.shape)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def model_run(ml_df, mll_df, starting_week, ship_min_week, ship_max_week, current_version, cat):\n",
    "    if len(ml_df) != 0:\n",
    "        logger.debug(\"ML Model is running\")\n",
    "        logger.debug(ml_df.shape)\n",
    "\n",
    "        ml_df.fillna(0, inplace=True)\n",
    "        ml_df = ml_df.sort_values(by=['PPG', 'Planning_Entity', 'SalesOrg', 'WEEK_NUM']).reset_index(drop=True)\n",
    "\n",
    "        remove = ['Shipment_Volume', 'PPG',\n",
    "                  'WEEK_NUM', 'Planning_Entity', 'SalesOrg', 'PRICE_REDUCTIONS_ONLY_PROMO_FLAG',\n",
    "                  'DISPLAY_ONLY_PROMO_FLAG',\n",
    "                  'FEATURE_DISPLAY_PROMO_FLAG', 'FEATURE_ONLY_PROMO_FLAG', 'weekno', 'weights', 'Year']\n",
    "\n",
    "\n",
    "        partial_remove = ['Shipment_Volume', 'PRICE_REDUCTIONS_ONLY_PROMO_FLAG',\n",
    "                  'DISPLAY_ONLY_PROMO_FLAG',\n",
    "                  'FEATURE_DISPLAY_PROMO_FLAG', 'FEATURE_ONLY_PROMO_FLAG', 'weekno', 'weights', 'Year']\n",
    "\n",
    "        if RunType == 'Forward':\n",
    "            weeklist = sorted(i for i in ml_df['WEEK_NUM'].unique().tolist() if i > ship_max_week)[:-4]\n",
    "        elif RunType == 'Backtest':\n",
    "            weeklist = [sorted(i for i in ml_df['WEEK_NUM'].unique().tolist() if i > ship_max_week)[3]]\n",
    "\n",
    "        logger.debug(\"Weeks to be forecasted\")\n",
    "        logger.debug(weeklist)\n",
    "\n",
    "        forecasted_xgb = []\n",
    "        forecasted_cat = []\n",
    "        forecasted_lgb = []\n",
    "        forecasted_rfm = []\n",
    "\n",
    "        ml_df['WEEK_NUM'] = ml_df['WEEK_NUM'].astype(np.int64)\n",
    "        train = ml_df[(ml_df['WEEK_NUM'] >= starting_week) & (ml_df['WEEK_NUM'] <= ship_max_week)]\n",
    "        test = ml_df[(ml_df['WEEK_NUM'] > ship_max_week)]\n",
    "\n",
    "        train['IsTrain'] = 1\n",
    "        test['IsTrain'] = 0\n",
    "        #######\n",
    "        ML_ADS = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "        train = train.drop(['IsTrain'], axis=1)\n",
    "        test = test.drop(['IsTrain'], axis=1)\n",
    "        ######\n",
    "\n",
    "        logger.debug(\"train and test shapes\")\n",
    "        logger.debug(train.shape)\n",
    "        logger.debug(test.shape)\n",
    "\n",
    "        ls1 = train['Planning_Entity'].unique().tolist()\n",
    "        test = test[test['Planning_Entity'].isin(ls1)]\n",
    "\n",
    "        ls2 = train['PPG'].unique().tolist()\n",
    "        test = test[test['PPG'].isin(ls2)]\n",
    "\n",
    "        dummy1 = pd.get_dummies(train['PPG'])\n",
    "        dummy2 = pd.get_dummies(train['Planning_Entity'])\n",
    "        # dropping zero columns\n",
    "        try:\n",
    "            dummy1.drop([0], axis=1, inplace=True)\n",
    "            dummy2.drop([0], axis=1, inplace=True)\n",
    "        except:\n",
    "            logger.debug(\"No zero column in dummy variables\")\n",
    "\n",
    "        train = pd.concat([train, dummy1, dummy2], axis=1)\n",
    "\n",
    "        train.replace(np.inf, np.nan, inplace=True)\n",
    "        train.fillna(0, inplace=True)\n",
    "\n",
    "        dummy1 = pd.get_dummies(test['PPG'])\n",
    "        dummy2 = pd.get_dummies(test['Planning_Entity'])\n",
    "        # dropping zero columns\n",
    "        try:\n",
    "            dummy1.drop([0], axis=1, inplace=True)\n",
    "            dummy2.drop([0], axis=1, inplace=True)\n",
    "        except:\n",
    "            logger.debug(\"No zero column in dummy variables\")\n",
    "\n",
    "        test = pd.concat([test, dummy1, dummy2], axis=1)\n",
    "\n",
    "        test.replace(np.inf, np.nan, inplace=True)\n",
    "        test.fillna(0, inplace=True)\n",
    "\n",
    "        cols = test.columns.to_list()\n",
    "        train = train[cols]\n",
    "\n",
    "        # creating recency based model weights on training set\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        logger.debug('Encoder input:')\n",
    "        logger.debug(train.WEEK_NUM.values)\n",
    "        train['weekno'] = le.fit_transform(train.WEEK_NUM.values)\n",
    "        logger.debug('Encoder output:')\n",
    "        logger.debug(train['weekno'].head())\n",
    "        train[\"weights\"] = 30 + (train[\"weekno\"] - train[\"weekno\"].max()) * np.exp(-2)\n",
    "        train[\"weights\"] = train[\"weights\"].astype(float)\n",
    "        train['weights'] = train['weights'] / train['weights'].max()\n",
    "        train.loc[(train.WEEK_NUM > 202005) & (train.WEEK_NUM < 202028), 'weights'] = 0.2\n",
    "        train.loc[(train.WEEK_NUM > 202010) & (train.WEEK_NUM < 202020), 'weights'] = 0.05\n",
    "\n",
    "        train_y1 = train['Shipment_Volume']\n",
    "        train_x1 = train[train.columns[~train.columns.isin(remove)]]\n",
    "\n",
    "        # 1 Xgboost Model\n",
    "        xgboostr = XGBRegressor(random_state=2017)\n",
    "        xgboostr.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        logger.debug('Xgboost Fitting completed successfully')\n",
    "\n",
    "        # 2 Catboost Model\n",
    "        cbr = CatBoostRegressor(random_state=2017)\n",
    "        cbr.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        # 3 lgbm Model\n",
    "        lgbr = LGBMRegressor(random_state=2017)\n",
    "        lgbr.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        # 4 rfm Model\n",
    "        rfm = RandomForestRegressor(random_state=2017)\n",
    "        rfm.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        models = {'xgboostr': xgboostr, 'cbr': cbr, 'lgbr': lgbr, 'rfm': rfm}\n",
    "\n",
    "        logger.debug('Fitting completed successfully')\n",
    "\n",
    "        try:\n",
    "            explainer_cbr = shap.TreeExplainer(cbr)\n",
    "            explainer_lgbr = shap.TreeExplainer(lgbr)\n",
    "            explainer_rfm = shap.TreeExplainer(rfm)\n",
    "            explainer_xgboostr = shap.TreeExplainer(xgboostr)\n",
    "\n",
    "            explainers = {'xgboostr': explainer_xgboostr, 'cbr': explainer_cbr, 'lgbr': explainer_lgbr, 'rfm': explainer_rfm}\n",
    "\n",
    "\n",
    "            start = time.time()\n",
    "            upload_pickle_to_azure(explainers, connect_str, tenant_name+ '/' + Folder_name + Pickle_subfolder_prefix + str(\n",
    "                ship_max_week) + '/explainers_' + str(cat) + '_US'+'.pkl', destination_name)\n",
    "            logger.debug('time taken to write in pickle file : ' + str(time.time() - start))\n",
    "\n",
    "            test_x = test[test.columns[~test.columns.isin(partial_remove)]]\n",
    "\n",
    "            start = time.time()\n",
    "            upload_ads_to_azure(test_x, connect_str,\n",
    "                                tenant_name + '/' + Folder_name + Parquet_subfolder_prefix + str(ship_max_week) + '/ML_fwd_' + str(cat) + '_US_test_ML',\n",
    "                                destination_name)\n",
    "            logger.debug('time taken to write in parquet format : ' + str(time.time() - start))\n",
    "            \n",
    "        except:\n",
    "            logger.debug('explainer failed to generate')\n",
    "\n",
    "\n",
    "        predict_start_time = time.time()\n",
    "\n",
    "        logger.debug(weeklist)\n",
    "        for i in weeklist:\n",
    "            logger.debug('i')\n",
    "            logger.debug(i)\n",
    "\n",
    "            test_1 = test[test['WEEK_NUM'] == i]\n",
    "\n",
    "            test_y1 = test_1['Shipment_Volume']\n",
    "            test_x1 = test_1[test_1.columns[~test_1.columns.isin(remove)]]\n",
    "\n",
    "            train_x1 = train_x1.fillna(0)\n",
    "            test_x1 = test_x1.fillna(0)\n",
    "\n",
    "            cols = train_x1.columns.to_list()\n",
    "            test_x1 = test_x1[cols]\n",
    "\n",
    "            logger.debug(\"week: {}\".format(i))\n",
    "            # XGBoost prediction\n",
    "            pred_xgb = xgboostr.predict(test_x1)\n",
    "            pred_xgb[pred_xgb < 0] = 0\n",
    "\n",
    "            testdata_xgb = test_1.copy()\n",
    "            testdata_xgb['Predicted'] = pred_xgb\n",
    "            testdata_xgb['Time.[Week]'] = i\n",
    "            testdata_xgb = testdata_xgb[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_xgb.append(testdata_xgb)\n",
    "\n",
    "            # Catboost prediction\n",
    "            pred_cat = cbr.predict(test_x1)\n",
    "            pred_cat[pred_cat < 0] = 0\n",
    "\n",
    "            testdata_cat = test_1.copy()\n",
    "            testdata_cat['Predicted'] = pred_cat\n",
    "            testdata_cat['Time.[Week]'] = i\n",
    "            testdata_cat = testdata_cat[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_cat.append(testdata_cat)\n",
    "\n",
    "            # lgbm prediction\n",
    "            pred_lgb = lgbr.predict(test_x1)\n",
    "            pred_lgb[pred_lgb < 0] = 0\n",
    "\n",
    "            testdata_lgb = test_1.copy()\n",
    "            testdata_lgb['Predicted'] = pred_lgb\n",
    "            testdata_lgb['Time.[Week]'] = i\n",
    "            testdata_lgb = testdata_lgb[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_lgb.append(testdata_lgb)\n",
    "\n",
    "            # Random forest prediction\n",
    "            pred_rfm = rfm.predict(test_x1)\n",
    "            pred_rfm[pred_rfm < 0] = 0\n",
    "\n",
    "            testdata_rfm = test_1.copy()\n",
    "            testdata_rfm['Predicted'] = pred_rfm\n",
    "            testdata_rfm['Time.[Week]'] = i\n",
    "            testdata_rfm = testdata_rfm[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_rfm.append(testdata_rfm)\n",
    "                \n",
    "        logger.debug('Predictions generated successfully')\n",
    "\n",
    "        predict_end_time = time.time()\n",
    "        predict_time = predict_end_time - predict_start_time\n",
    "        logger.debug('predict_time: {}'.format(predict_time))\n",
    "\n",
    "        logger.debug('Generated weeks: {}'.format(testdata_rfm['Time.[Week]'].unique().tolist()))\n",
    "\n",
    "        # Generating Xgboost output###\n",
    "        df_results_xgb = model_results(forecasted_xgb, current_version)\n",
    "        # Generating Catboost output###\n",
    "        df_results_cat = model_results(forecasted_cat, current_version)\n",
    "        # Generating LGBM output###\n",
    "        df_results_lgb = model_results(forecasted_lgb, current_version)\n",
    "        # Generating RFM output###\n",
    "        df_results_rfm = model_results(forecasted_rfm, current_version)\n",
    "\n",
    "        if RunType == 'Forward':\n",
    "        \n",
    "            try:\n",
    "            # generating feature importance\n",
    "                feat_imp_df_cbr = feat_imp(model=cbr, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=True, cat=cat)\n",
    "                feat_imp_df_lgbr = feat_imp(model=lgbr, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=True, cat=cat)\n",
    "                feat_imp_df_rfm = feat_imp(model=rfm, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=True, cat=cat)\n",
    "                feat_imp_df_xgboostr = feat_imp(model=xgboostr, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=True, cat=cat)\n",
    "\n",
    "                                                                                                            \n",
    "                concat_grain = ['Version.[Version Name]','Time.[Week]','Item.[L6]','Sales Domain.[Sales Org]','[ML Driver].[ML Driver]']\n",
    "                feat_imp_df_cbr = feat_imp_df_cbr.set_index(concat_grain)\n",
    "                feat_imp_df_lgbr = feat_imp_df_lgbr.set_index(concat_grain)\n",
    "                feat_imp_df_rfm = feat_imp_df_rfm.set_index(concat_grain)\n",
    "                feat_imp_df_xgboostr = feat_imp_df_xgboostr.set_index(concat_grain)\n",
    "\n",
    "                feat_imp_df = pd.concat([feat_imp_df_cbr,feat_imp_df_lgbr,feat_imp_df_rfm,feat_imp_df_xgboostr],axis=1).reset_index()\n",
    "\n",
    "            except:\n",
    "                feat_imp_df = pd.DataFrame()\n",
    "                logger.debug('No feat_imp_df generated')                \n",
    "\n",
    "\n",
    "    # MLL modeling\n",
    "    if len(mll_df) != 0:\n",
    "        logger.debug(\"MLL Model running\")\n",
    "\n",
    "        mll_df.fillna(0, inplace=True)\n",
    "        mll_df = mll_df.sort_values(by=['PPG', 'Planning_Entity', 'SalesOrg', 'WEEK_NUM']).reset_index(drop=True)\n",
    "\n",
    "        remove = ['Shipment_Volume', 'PPG', 'WEEK_NUM', 'Planning_Entity', 'SalesOrg',\n",
    "                  'Distribution', 'DisplayOnlyPrice',\n",
    "                  'FeatureOnlyPrice', 'FeatureandDisplayPrice',\n",
    "                  'PriceReductionPrice', 'StandardPrice', 'DISPLAY_ONLY_PROMO_FLAG',\n",
    "                  'FEATURE_DISPLAY_PROMO_FLAG', 'FEATURE_ONLY_PROMO_FLAG',\n",
    "                  'PRICE_REDUCTIONS_ONLY_PROMO_FLAG', 'DISPLAY_ONLY_PROMO_FLAG_lead2',\n",
    "                  'DISPLAY_ONLY_PROMO_FLAG_lead3', 'FEATURE_DISPLAY_PROMO_FLAG_lead2',\n",
    "                  'FEATURE_DISPLAY_PROMO_FLAG_lead3', 'FEATURE_ONLY_PROMO_FLAG_lead2',\n",
    "                  'FEATURE_ONLY_PROMO_FLAG_lead3',\n",
    "                  'PRICE_REDUCTIONS_ONLY_PROMO_FLAG_lead2',\n",
    "                  'PRICE_REDUCTIONS_ONLY_PROMO_FLAG_lead3', 'weekno', 'weights', 'Year']\n",
    "\n",
    "        partial_remove = ['Shipment_Volume',\n",
    "                  'Distribution', 'DisplayOnlyPrice',\n",
    "                  'FeatureOnlyPrice', 'FeatureandDisplayPrice',\n",
    "                  'PriceReductionPrice', 'StandardPrice', 'DISPLAY_ONLY_PROMO_FLAG',\n",
    "                  'FEATURE_DISPLAY_PROMO_FLAG', 'FEATURE_ONLY_PROMO_FLAG',\n",
    "                  'PRICE_REDUCTIONS_ONLY_PROMO_FLAG', 'DISPLAY_ONLY_PROMO_FLAG_lead2',\n",
    "                  'DISPLAY_ONLY_PROMO_FLAG_lead3', 'FEATURE_DISPLAY_PROMO_FLAG_lead2',\n",
    "                  'FEATURE_DISPLAY_PROMO_FLAG_lead3', 'FEATURE_ONLY_PROMO_FLAG_lead2',\n",
    "                  'FEATURE_ONLY_PROMO_FLAG_lead3',\n",
    "                  'PRICE_REDUCTIONS_ONLY_PROMO_FLAG_lead2',\n",
    "                  'PRICE_REDUCTIONS_ONLY_PROMO_FLAG_lead3', 'weekno', 'weights', 'Year']\n",
    "\n",
    "        if RunType == 'Forward':\n",
    "            weeklist = sorted(i for i in mll_df['WEEK_NUM'].unique().tolist() if i > ship_max_week)[:-4]\n",
    "        elif RunType == 'Backtest':\n",
    "            weeklist = [sorted(i for i in mll_df['WEEK_NUM'].unique().tolist() if i > ship_max_week)[3]]\n",
    "        logger.debug(\"Weeks to be forecasted for MLL\")\n",
    "        logger.debug(weeklist)\n",
    "\n",
    "        forecasted_xgb = []\n",
    "        forecasted_cat = []\n",
    "        forecasted_lgb = []\n",
    "        forecasted_rfm = []\n",
    "\n",
    "        train = mll_df[(mll_df['WEEK_NUM'] >= starting_week) & (mll_df['WEEK_NUM'] <= ship_max_week)]\n",
    "        test = mll_df[(mll_df['WEEK_NUM'] > ship_max_week)]\n",
    "\n",
    "        train['IsTrain'] = 1\n",
    "        test['IsTrain'] = 0\n",
    "        MLL_ADS = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "        train = train.drop(['IsTrain'], axis=1)\n",
    "        test = test.drop(['IsTrain'], axis=1)\n",
    "        #######\n",
    "        try:\n",
    "            MLL_ADS = pd.concat([ML_ADS, MLL_ADS], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            logger.debug('no drive data for ' + cat)\n",
    "\n",
    "        #######\n",
    "        for i in MLL_ADS.columns:\n",
    "            MLL_ADS[i] = MLL_ADS[i].astype(str)\n",
    "\n",
    "        logger.debug('Writing MLL+ML df')\n",
    "        n = MLL_ADS.shape[0]\n",
    "        logger.debug('Number of records to write:' + str(n))\n",
    "        start = time.time()\n",
    "        upload_ads_to_azure(MLL_ADS, connect_str,\n",
    "                            tenant_name + '/' + Folder_name + Parquet_subfolder_prefix + str(ship_max_week) + '/ML_fwd_' + str(cat) + '_US',\n",
    "                            destination_name)\n",
    "        logger.debug('time taken to write in parquet format : ' + str(time.time() - start))\n",
    "\n",
    "        ls1 = train['Planning_Entity'].unique().tolist()\n",
    "        test = test[test['Planning_Entity'].isin(ls1)]\n",
    "\n",
    "        ls2 = train['PPG'].unique().tolist()\n",
    "        test = test[test['PPG'].isin(ls2)]\n",
    "\n",
    "        dummy1 = pd.get_dummies(train['PPG'])\n",
    "        dummy2 = pd.get_dummies(train['Planning_Entity'])\n",
    "\n",
    "        # dropping zero columns\n",
    "        try:\n",
    "            dummy1.drop([0], axis=1, inplace=True)\n",
    "            dummy2.drop([0], axis=1, inplace=True)\n",
    "        except:\n",
    "            logger.debug(\"No zero column in dummy variables\")\n",
    "\n",
    "        train = pd.concat([train, dummy1, dummy2], axis=1)\n",
    "\n",
    "        train.replace(np.inf, np.nan, inplace=True)\n",
    "        train.fillna(0, inplace=True)\n",
    "\n",
    "        try:\n",
    "            train.drop([0], axis=1, inplace=True)\n",
    "        except:\n",
    "            logger.debug(\"No zero column in train data\")\n",
    "\n",
    "        dummy1 = pd.get_dummies(test['PPG'])\n",
    "        dummy2 = pd.get_dummies(test['Planning_Entity'])\n",
    "\n",
    "        # dropping zero columns\n",
    "        try:\n",
    "            dummy1.drop([0], axis=1, inplace=True)\n",
    "            dummy2.drop([0], axis=1, inplace=True)\n",
    "        except:\n",
    "            logger.debug(\"No zero column in dummy variables\")\n",
    "\n",
    "        test = pd.concat([test, dummy1, dummy2], axis=1)\n",
    "\n",
    "        test.replace(np.inf, np.nan, inplace=True)\n",
    "        test.fillna(0, inplace=True)\n",
    "\n",
    "        try:\n",
    "            test.drop([0], axis=1, inplace=True)\n",
    "        except:\n",
    "            logger.debug(\"No zero column in test data\")\n",
    "\n",
    "        cols = test.columns.to_list()\n",
    "        train = train[cols]\n",
    "\n",
    "        # creating recency based model weights on training set\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        logger.debug('Encoder input:')\n",
    "        logger.debug(train.WEEK_NUM.values)\n",
    "        train['weekno'] = le.fit_transform(train.WEEK_NUM.values)\n",
    "        logger.debug('Encoder output:')\n",
    "        logger.debug(train['weekno'].head())\n",
    "        train[\"weights\"] = 30 + (train[\"weekno\"] - train[\"weekno\"].max()) * np.exp(-2)\n",
    "        train[\"weights\"] = train[\"weights\"].astype(float)\n",
    "        train['weights'] = train['weights'] / train['weights'].max()\n",
    "        train.loc[(train.WEEK_NUM > 202005) & (train.WEEK_NUM < 202028), 'weights'] = 0.2\n",
    "        train.loc[(train.WEEK_NUM > 202010) & (train.WEEK_NUM < 202020), 'weights'] = 0.05\n",
    "\n",
    "        train_y1 = train['Shipment_Volume']\n",
    "        train_x1 = train[train.columns[~train.columns.isin(remove)]]\n",
    "\n",
    "        # 1 Xgboost Model\n",
    "        xgboostr = XGBRegressor(random_state=2017)\n",
    "        xgboostr.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        logger.debug('Xgboost Fitting completed successfully')\n",
    "\n",
    "        # 2 Catboost Model\n",
    "        cbr = CatBoostRegressor(random_state=2017)\n",
    "        cbr.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        # 3 lgbm Model\n",
    "        lgbr = LGBMRegressor(random_state=2017)\n",
    "        lgbr.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        # 4 rfm Model\n",
    "        rfm = RandomForestRegressor(random_state=2017)\n",
    "        rfm.fit(train_x1, train_y1, sample_weight=train[\"weights\"].to_list())\n",
    "\n",
    "        models_MLL = {'xgboostr_driver_less': xgboostr, 'cbr_driver_less': cbr, 'lgbr_driver_less': lgbr,\n",
    "                      'rfm_driver_less': rfm}\n",
    "        if 'models' in locals():\n",
    "            models_MLL = {**models,**models_MLL}\n",
    "\n",
    "        if Pickle == 'Yes':\n",
    "            start = time.time()\n",
    "            upload_pickle_to_azure(models_MLL, connect_str, tenant_name+ '/' + Folder_name + Pickle_subfolder_prefix + str(\n",
    "                ship_max_week) + '/models_' + cat + '_US'+'.pkl', destination_name)\n",
    "            logger.debug('time taken to write in pickle file : ' + str(time.time() - start))\n",
    "\n",
    "        logger.debug('Fitting completed successfully')\n",
    "\n",
    "        try:\n",
    "\n",
    "            explainer_cbr_driverless = shap.TreeExplainer(cbr)\n",
    "            explainer_lgbr_driverless = shap.TreeExplainer(lgbr)\n",
    "            explainer_rfm_driverless = shap.TreeExplainer(rfm)\n",
    "            explainer_xgboostr_driverless = shap.TreeExplainer(xgboostr)\n",
    "\n",
    "            explainers_driverless = {'xgboostr_driver_less': explainer_xgboostr_driverless, 'cbr_driver_less': explainer_cbr_driverless, 'lgbr_driver_less': explainer_lgbr_driverless,\n",
    "                          'rfm_driver_less': explainer_rfm_driverless}\n",
    "\n",
    "            start = time.time()\n",
    "            upload_pickle_to_azure(explainers_driverless, connect_str, tenant_name+ '/' + Folder_name + Pickle_subfolder_prefix + str(\n",
    "                ship_max_week) + '/explainers_driverless_' + cat + '_US'+'.pkl', destination_name)\n",
    "            logger.debug('time taken to write in pickle file : ' + str(time.time() - start))\n",
    "\n",
    "            test_x = test[test.columns[~test.columns.isin(partial_remove)]]\n",
    "\n",
    "            start = time.time()\n",
    "            upload_ads_to_azure(test_x, connect_str,\n",
    "                                tenant_name + '/' + Folder_name + Parquet_subfolder_prefix + str(ship_max_week) + '/ML_fwd_' + str(cat) + '_US_test_MLL',\n",
    "                                destination_name)\n",
    "            logger.debug('time taken to write in parquet format : ' + str(time.time() - start))\n",
    "\n",
    "        except:\n",
    "            logger.debug('explainer_driverless failed to generate')\n",
    "\n",
    "        predict_start_time = time.time()\n",
    "\n",
    "        logger.debug(weeklist)\n",
    "        for i in weeklist:\n",
    "            logger.debug('i')\n",
    "            logger.debug(i)\n",
    "\n",
    "            test_1 = test[test['WEEK_NUM'] == i]\n",
    "\n",
    "            test_y1 = test_1['Shipment_Volume']\n",
    "            test_x1 = test_1[test_1.columns[~test_1.columns.isin(remove)]]\n",
    "\n",
    "            train_x1 = train_x1.fillna(0)\n",
    "            test_x1 = test_x1.fillna(0)\n",
    "\n",
    "            cols = train_x1.columns.to_list()\n",
    "            test_x1 = test_x1[cols]\n",
    "\n",
    "            logger.debug(\"week: {}\".format(i))\n",
    "            # XGBoost prediction\n",
    "            pred_xgb = xgboostr.predict(test_x1)\n",
    "            pred_xgb[pred_xgb < 0] = 0\n",
    "\n",
    "            testdata_xgb = test_1.copy()\n",
    "            testdata_xgb['Predicted'] = pred_xgb\n",
    "            testdata_xgb['Time.[Week]'] = i\n",
    "            testdata_xgb = testdata_xgb[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_xgb.append(testdata_xgb)\n",
    "\n",
    "            # Catboost prediction\n",
    "            pred_cat = cbr.predict(test_x1)\n",
    "            pred_cat[pred_cat < 0] = 0\n",
    "\n",
    "            testdata_cat = test_1.copy()\n",
    "            testdata_cat['Predicted'] = pred_cat\n",
    "            testdata_cat['Time.[Week]'] = i\n",
    "            testdata_cat = testdata_cat[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_cat.append(testdata_cat)\n",
    "\n",
    "            # lgbm prediction\n",
    "            pred_lgb = lgbr.predict(test_x1)\n",
    "            pred_lgb[pred_lgb < 0] = 0\n",
    "\n",
    "            testdata_lgb = test_1.copy()\n",
    "            testdata_lgb['Predicted'] = pred_lgb\n",
    "            testdata_lgb['Time.[Week]'] = i\n",
    "            testdata_lgb = testdata_lgb[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_lgb.append(testdata_lgb)\n",
    "\n",
    "            # Random forest prediction\n",
    "            pred_rfm = rfm.predict(test_x1)\n",
    "            pred_rfm[pred_rfm < 0] = 0\n",
    "\n",
    "            testdata_rfm = test_1.copy()\n",
    "            testdata_rfm['Predicted'] = pred_rfm\n",
    "            testdata_rfm['Time.[Week]'] = i\n",
    "            testdata_rfm = testdata_rfm[['WEEK_NUM', 'PPG', 'Planning_Entity', 'Time.[Week]', 'SalesOrg', 'Predicted']]\n",
    "\n",
    "            forecasted_rfm.append(testdata_rfm)\n",
    "            \n",
    "        logger.debug('Predictions generated successfully')\n",
    "\n",
    "        predict_end_time = time.time()\n",
    "        predict_time = predict_end_time - predict_start_time\n",
    "        logger.debug('predict_time (MLL): {}'.format(predict_time))\n",
    "\n",
    "        # Generating Xgboost output #\n",
    "        df_results_xgb1 = model_results(forecasted_xgb, current_version)\n",
    "        # Generating Catboost output #\n",
    "        df_results_cat1 = model_results(forecasted_cat, current_version)\n",
    "        # Generating LGBM output #\n",
    "        df_results_lgb1 = model_results(forecasted_lgb, current_version)\n",
    "        # Generating RFM output #\n",
    "        df_results_rfm1 = model_results(forecasted_rfm, current_version)\n",
    "            \n",
    "        if RunType == 'Forward':\n",
    "        \n",
    "            try:\n",
    "            \n",
    "                # Generating FI for MLL mode\n",
    "                feat_imp_df1_cbr = feat_imp(model=cbr, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=False, cat=cat)\n",
    "                feat_imp_df1_lgbr = feat_imp(model=lgbr, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=False, cat=cat)\n",
    "                feat_imp_df1_rfm = feat_imp(model=rfm, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=False, cat=cat)\n",
    "                feat_imp_df1_xgboostr = feat_imp(model=xgboostr, train_x1=train_x1, current_version=current_version,\n",
    "                                       ship_max_week=ship_max_week, featflag=False, cat=cat)\n",
    "\n",
    "                concat_grain = ['Version.[Version Name]','Time.[Week]','Item.[L6]','Sales Domain.[Sales Org]','[ML Driver].[ML Driver]']\n",
    "                feat_imp_df1_cbr = feat_imp_df1_cbr.set_index(concat_grain)\n",
    "                feat_imp_df1_lgbr = feat_imp_df1_lgbr.set_index(concat_grain)\n",
    "                feat_imp_df1_rfm = feat_imp_df1_rfm.set_index(concat_grain)\n",
    "                feat_imp_df1_xgboostr = feat_imp_df1_xgboostr.set_index(concat_grain)\n",
    "\n",
    "                feat_imp_df1 = pd.concat([feat_imp_df1_cbr,feat_imp_df1_lgbr,feat_imp_df1_rfm,feat_imp_df1_xgboostr],axis=1).reset_index()\n",
    "\n",
    "            except:\n",
    "                feat_imp_df1 = pd.DataFrame()\n",
    "                logger.debug('feat_imp_df1 not generated')\n",
    "\n",
    "    # Final result for Catboost (Model 1 in tenant) #\n",
    "    if len(ml_df) != 0:\n",
    "        logger.debug(\"ML+MLL Result\")\n",
    "        try:\n",
    "            actual_results_cat = pd.concat([df_results_cat, df_results_cat1], ignore_index=True)\n",
    "        except:\n",
    "            actual_results_cat = df_results_cat\n",
    "\n",
    "        actual_results_cat = actual_results_cat.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_cat.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_cat.rename(columns={'Predicted': 'ML Model 1 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_cat.rename(columns={'Predicted': 'ML Model 1 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model1 = actual_results_cat.copy()\n",
    "\n",
    "        # Final result for LGBM (Model 2 in tenant) #\n",
    "        try:\n",
    "            actual_results_lgb = pd.concat([df_results_lgb, df_results_lgb1], ignore_index=True)\n",
    "        except:\n",
    "            actual_results_lgb = df_results_lgb\n",
    "\n",
    "        actual_results_lgb = actual_results_lgb.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_lgb.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_lgb.rename(columns={'Predicted': 'ML Model 2 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_lgb.rename(columns={'Predicted': 'ML Model 2 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model2 = actual_results_lgb.copy()\n",
    "\n",
    "        # Final result for Random Forest Model (Model 3 in tenant) #\n",
    "        try:\n",
    "            actual_results_rfm = pd.concat([df_results_rfm, df_results_rfm1], ignore_index=True)\n",
    "        except:\n",
    "            actual_results_rfm = df_results_rfm\n",
    "\n",
    "        actual_results_rfm = actual_results_rfm.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_rfm.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_rfm.rename(columns={'Predicted': 'ML Model 3 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_rfm.rename(columns={'Predicted': 'ML Model 3 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model3 = actual_results_rfm.copy()\n",
    "\n",
    "        # Final result for Xgboost Model (Model 4 in tenant) #\n",
    "        try:\n",
    "            actual_results_xgb = pd.concat([df_results_xgb, df_results_xgb1], ignore_index=True)\n",
    "        except:\n",
    "            actual_results_xgb = df_results_xgb\n",
    "\n",
    "        actual_results_xgb = actual_results_xgb.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_xgb.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_xgb.rename(columns={'Predicted': 'ML Model 4 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_xgb.rename(columns={'Predicted': 'ML Model 4 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model4 = actual_results_xgb.copy()\n",
    "            \n",
    "        if RunType == 'Forward':\n",
    "            return actual_model1, actual_model2, actual_model3, actual_model4, feat_imp_df\n",
    "        else:\n",
    "            return actual_model1, actual_model2, actual_model3, actual_model4       \n",
    "\n",
    "    else:\n",
    "        logger.debug(\"FS Result\")\n",
    "        actual_results_cat = df_results_cat1\n",
    "\n",
    "        actual_results_cat = actual_results_cat.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_cat.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_cat.rename(columns={'Predicted': 'ML Model 1 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_cat.rename(columns={'Predicted': 'ML Model 1 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model1 = actual_results_cat.copy()\n",
    "\n",
    "        # Final result for LGBM (Model 2 in tenant) #\n",
    "        actual_results_lgb = df_results_lgb1\n",
    "\n",
    "        actual_results_lgb = actual_results_lgb.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_lgb.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_lgb.rename(columns={'Predicted': 'ML Model 2 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_lgb.rename(columns={'Predicted': 'ML Model 2 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model2 = actual_results_lgb.copy()\n",
    "\n",
    "        # Final result for Random Forest Model (Model 3 in tenant) #\n",
    "        actual_results_rfm = df_results_rfm1\n",
    "\n",
    "        actual_results_rfm = actual_results_rfm.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_rfm.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_rfm.rename(columns={'Predicted': 'ML Model 3 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_rfm.rename(columns={'Predicted': 'ML Model 3 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model3 = actual_results_rfm.copy()\n",
    "\n",
    "        # Final result for Xgboost Model (Model 4 in tenant) #\n",
    "        actual_results_xgb = df_results_xgb1\n",
    "\n",
    "        actual_results_xgb = actual_results_xgb.groupby(['Version.[Version Name]', 'Item.[L2]',\n",
    "                                                         'Sales Domain.[Customer Group]', 'Time.[Week]', 'SalesOrg'])[\n",
    "            'Predicted'].sum().reset_index().drop_duplicates()\n",
    "\n",
    "        actual_results_xgb.drop(['SalesOrg'], axis=1, inplace=True)\n",
    "        if RunType == 'Forward':\n",
    "            actual_results_xgb.rename(columns={'Predicted': 'ML Model 4 Fcst HL'}, inplace=True)\n",
    "        elif RunType == 'Backtest':\n",
    "            actual_results_xgb.rename(columns={'Predicted': 'ML Model 4 Fcst HL Lag2'}, inplace=True)\n",
    "        actual_model4 = actual_results_xgb.copy()\n",
    "        \n",
    "        if RunType == 'Forward':\n",
    "            return actual_model1, actual_model2, actual_model3, actual_model4, feat_imp_df1\n",
    "        else:\n",
    "            return actual_model1, actual_model2, actual_model3, actual_model4 \n",
    "\n",
    "\n",
    "ml_df, mll_df, starting_week, ship_max_week, ship_min_week = ads(shipment_df=shipment_df,\n",
    "                                                                 imputed_price_promo_df=imputed_price_promo_df,\n",
    "                                                                                                   \n",
    "                                                                 year_week_df=time_df, calendar_df=calendar_df,\n",
    "                                                                 uom_df=uom_df, compass_ppg=compass_ppg)\n",
    "\n",
    "if RunType=='Forward':\n",
    "    result1, result2, result3, result4, feat = model_run(ml_df=ml_df, mll_df=mll_df, starting_week=starting_week,\n",
    "                                                              ship_max_week=ship_max_week, ship_min_week=ship_min_week,\n",
    "                                                              current_version=current_version, cat=cat)\n",
    "else:\n",
    "    result1, result2, result3, result4 = model_run(ml_df=ml_df, mll_df=mll_df, starting_week=starting_week,\n",
    "                                                              ship_max_week=ship_max_week, ship_min_week=ship_min_week,\n",
    "                                                              current_version=current_version, cat=cat)\n",
    "\n",
    "if RunType == 'Forward':\n",
    "    logger.debug(result1.groupby('Time.[Week]')['ML Model 1 Fcst HL'].sum().astype(np.int))\n",
    "    logger.debug(result2.groupby('Time.[Week]')['ML Model 2 Fcst HL'].sum().astype(np.int))\n",
    "    logger.debug(result3.groupby('Time.[Week]')['ML Model 3 Fcst HL'].sum().astype(np.int))\n",
    "    logger.debug(result4.groupby('Time.[Week]')['ML Model 4 Fcst HL'].sum().astype(np.int))\n",
    "elif RunType == 'Backtest':\n",
    "    logger.debug(result1.groupby('Time.[Week]')['ML Model 1 Fcst HL Lag2'].sum().astype(np.int))\n",
    "    logger.debug(result2.groupby('Time.[Week]')['ML Model 2 Fcst HL Lag2'].sum().astype(np.int))\n",
    "    logger.debug(result3.groupby('Time.[Week]')['ML Model 3 Fcst HL Lag2'].sum().astype(np.int))\n",
    "    logger.debug(result4.groupby('Time.[Week]')['ML Model 4 Fcst HL Lag2'].sum().astype(np.int))\n",
    "\n",
    "logger.debug(\"Shape of Model 1 result\")\n",
    "logger.debug(result1.shape)\n",
    "logger.debug(\"Sum of Model 1 predicted result\")\n",
    "if RunType == 'Forward':\n",
    "    logger.debug(result1['ML Model 1 Fcst HL'].sum())\n",
    "elif RunType == 'Backtest':\n",
    "    logger.debug(result1['ML Model 1 Fcst HL Lag2'].sum())\n",
    "logger.debug(\"Shape of Model 2 result\")\n",
    "logger.debug(result2.shape)\n",
    "logger.debug(\"Shape of Model 3 result\")\n",
    "logger.debug(result2.shape)\n",
    "logger.debug(\"Shape of Model 4 result\")\n",
    "logger.debug(result4.shape)\n",
    "\n",
    "Actual_model = reduce(lambda x, y: pd.merge(x, y,\n",
    "                                            on=['Version.[Version Name]', 'Item.[L2]', 'Sales Domain.[Customer Group]',\n",
    "                                                'Time.[Week]'], how='left'), [result1, result2, result3, result4])\n",
    "\n",
    "Feat_model = pd.DataFrame()                                \n",
    "if RunType == 'Forward':\n",
    "    try:\n",
    "        Feat_model = feat\n",
    "        logger.debug(Feat_model['Driver Importance Model 1'].sum())\n",
    "    except:\n",
    "        logger.debug('No feat output generated')\n",
    "    \n",
    "\n",
    "logger.debug(\"Actual_model.Shape\")\n",
    "logger.debug(Actual_model.shape)\n",
    "logger.debug(\"Actual_model.head()\")\n",
    "logger.debug(Actual_model.head())\n",
    "\n",
    "###################################################\n",
    "logger.debug(\"KHC ML Fwd-Backtest script completed...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbe37a-3f5d-4c1e-8580-7d975075f46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[ProphesyInt] Tenant Conda Environment",
   "language": "python",
   "name": "genieaz_prophesyint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notebook_dict": {
   "ClassName": "o9.GraphCube.Plugins.Python.PythonScript",
   "InstanceName": "KHC_US_ML_Fwd_Looking_Final_112wks",
   "SliceKeys": [
    {
     "AttributeName": "L6",
     "DimensionName": "Item"
    }
   ],
   "file_path": "loaded_notebooks/KHC_US_ML_Fwd_Looking_Final_112wks.ipynb",
   "o9_selected_plugin_id": 203265
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
